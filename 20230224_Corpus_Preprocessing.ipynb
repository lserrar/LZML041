{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0f26fb-8a7c-4439-8a68-fdc2c5fde9cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# étape 1: Installation NLTK et librairies de visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7083b1-3e44-4560-896b-501186e1fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 1... installer NLKT si vous ne l'avez pas déjà fait \n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846728a6-bcea-494e-b308-107d0df3d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1... import the library NLKT\n",
    "# A new window should open, showing the NLTK Downloader. Click on the File menu and select Change Download Directory. \n",
    "# For central installation, set this to C:\\nltk_data (Windows), /usr/local/share/nltk_data (Mac), or /usr/share/nltk_data (Unix). \n",
    "# Next, download all.\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba3d3e-28fb-4236-8053-c6915be56a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2....installer visualisation libraries\n",
    "!{sys.executable} -m pip install networkx[default]\n",
    "!{sys.executable} -m pip install pyvis\n",
    "!{sys.executable} -m pip install textblob\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c9cc5-4cbd-4868-8304-9474eb82fb56",
   "metadata": {},
   "source": [
    "# Corpus - example 1 : Wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf4cbd-e870-4ff0-9d6c-e7795ebee4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage for wordnet\n",
    "# WordNet is just a NLTK corpus reader, and can be imported like this:\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70156302-9961-4be7-aec9-68d4679eb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"lemme\"\n",
    "synset = wn.synsets(word, lang='fra')\n",
    "print('The test word is : ', word)\n",
    "print('Word and Type : ' + synset[0].name())\n",
    "print('Synonym is: ' + synset[0].lemmas()[0].name())\n",
    "print('The meaning of the word : ' + synset[0].definition())\n",
    "print('Example : ' + str(synset[0].examples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeb7bc-78c4-4315-b0f8-6403eb5e5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "noeud = Word(\"word\")\n",
    "print (noeud.synsets[:10])\n",
    "print (noeud.definitions[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5489ad-ae6a-45a6-8a96-2778c613bcab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G=nx.Graph()\n",
    "\n",
    "w=noeud.synsets[0]\n",
    "\n",
    "G.add_node(w.name())\n",
    "for h in w.hypernyms():\n",
    "    #print (h)\n",
    "    G.add_node(h.name())\n",
    "    G.add_edge(w.name(),h.name())\n",
    "\n",
    "\n",
    "for h in w.hyponyms():\n",
    "    #print (h)\n",
    "    G.add_node(h.name())\n",
    "    G.add_edge(w.name(),h.name())\n",
    "\n",
    "print (G.nodes(data=True))\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = [21, 5]\n",
    "nx.draw(G, width=1, with_labels=True, node_color=\"#007ed9\")\n",
    "plt.savefig(\"path.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd849e8-4c65-426d-b66a-0e4cc0494845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see all other layouts: https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.random_layout.html\n",
    "nx.draw(G, pos=nx.spiral_layout(G))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284151ca-31ee-45ef-9731-31c5ae123cb9",
   "metadata": {},
   "source": [
    "# Corpus - example 2 : Guttenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33632eb-9db2-4564-9643-abace604282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore the Guttenberg in NLKT !\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fa038-7da3-4a2f-a361-22f018791027",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick = nltk.corpus.gutenberg.words( 'melville-moby_dick.txt')\n",
    "len(moby_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d41410-e269-41b3-93f7-11bae84fcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in nltk.corpus.gutenberg.fileids():\n",
    "    print('# of words in ',text,'is: ', len(nltk.corpus.gutenberg.words( text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006f44e-3e0e-4ae7-aa96-728189205250",
   "metadata": {},
   "source": [
    "## a- Télécharger un livre du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbcde9-1083-47e5-9514-f6ae77edda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load a specific book from Guttenberg website \n",
    "# you will need to leverage the requests package\n",
    "import requests\n",
    "#choose a book in Gutenberg project website the https://www.gutenberg.org/ebooks/5258 and get the reference number of the book, here 5258 !\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/5258/pg5258.txt')\n",
    "Zarathoustra_Nietzsche = r.text\n",
    "\n",
    "# first, remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "    Zarathoustra_Nietzsche = Zarathoustra_Nietzsche.replace(char, \" \")\n",
    "#print number of characters in the book\n",
    "print(len(Zarathoustra_Nietzsche))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebed35-bfe4-43ad-b036-ab35ddbfc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the project gutenburg introduction and footnotes\n",
    "print(Zarathoustra_Nietzsche[0:910]) \n",
    "print('-------------------------------------------------') \n",
    "print(Zarathoustra_Nietzsche[637986:639986]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86566f-a51a-4438-a044-84b764220280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also subset for the book text\n",
    "# (removing the project gutenburg introduction/footnotes)\n",
    "Zarathoustra_Nietzsche = Zarathoustra_Nietzsche[911:637986]\n",
    "#print(Zarathoustra_Nietzsche)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c6e95-07ed-4efe-a600-284104f0cacb",
   "metadata": {},
   "source": [
    "## b- Explorer le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b12172-37cf-474e-9df5-2dba7c702250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Tokenize the Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "# Choose your Text\n",
    "text = Zarathoustra_Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687cd3d-6e01-448b-8153-536111ee5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =word_tokenize(text, language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a05c4-a370-459f-8e5e-c27a6d13d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 20 most commons tokens\n",
    "from collections import Counter\n",
    "print(Counter(tokens).most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e262d-cd52-4d2c-97e5-602853fd5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove punctiation !\n",
    "remove = re.sub(r'[^\\w\\s]', '', text)\n",
    "#print(\"updated text with no punctuations :\", remove)\n",
    "tokens =word_tokenize(remove, language=\"french\")\n",
    "print(Counter(tokens).most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344a87f-5034-4868-8bcb-d6610e2c364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=100,\n",
    "                      random_state=1, background_color='White', colormap='cubehelix',\n",
    "                      collocations=False, stopwords = STOPWORDS).generate(text)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a68002-8a2c-4412-b720-97d245b69ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove Stopwords !\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "filtre_stopfr =  lambda text: [token for token in text if token.lower() not in french_stopwords]\n",
    "\n",
    "tokens_Filtered=filtre_stopfr( tokens)\n",
    "print(Counter(tokens_Filtered).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc803de5-b9d7-4a40-a667-e71bce661a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=100,\n",
    "                      random_state=1, background_color='Black', colormap='Paired',\n",
    "                      collocations=False, stopwords = french_stopwords).generate(text)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f4ac6-c6d4-4b80-9cb3-6209ab94f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "mask = np.array(Image.open(\"Nietzsche.jpg\"))\n",
    "mask.shape\n",
    "# Generating colors from image\n",
    "image_colors = ImageColorGenerator(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39ef12-3ca2-44a5-bb16-55dabb288d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 800, max_words=700,\n",
    "                      random_state=1, background_color='Lightblue', colormap='winter_r',\n",
    "                      collocations=False, stopwords = french_stopwords, mask = mask).generate(text)\n",
    "plt.figure(figsize=(5, 13))\n",
    "plt.imshow(wordcloud) \n",
    "#plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation ='bilinear') # Using the color function to use the image colors\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Pour afficher l'image mask...\n",
    "#plt.imshow(mask) \n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537f5cd-a964-4a67-a32d-4293df7ec083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you wantg to exclude anything else?\n",
    "Stop_words=['plus']\n",
    "for x in Stop_words:\n",
    "    french_stopwords.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af59814-dc03-4f73-957c-a993dec3926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_Filtered=filtre_stopfr( tokens)\n",
    "print(Counter(tokens_Filtered).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7ce72-81e5-4894-a288-d3951b9099cd",
   "metadata": {},
   "source": [
    "# Corpus example 4 : Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8026c-ea1a-49f3-9c5f-6dac1e670193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snscrape is a scraper for social networking services (SNS). It scrapes things like user profiles, hashtags, or searches and returns the discovered items, e.g. the relevant posts. \n",
    "!{sys.executable} -m pip install snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4255c5-8c94-4589-8845-947719fb0bee",
   "metadata": {},
   "source": [
    "## Web scraping : Récupérer les tweets directement d'internet\n",
    "snscrape is a scraper for social networking services (SNS). It scrapes things like user profiles, hashtags, or searches and returns the discovered items, e.g. the relevant posts, from various services: facoebook, instagram, Twitter....\n",
    "\n",
    "#https://github.com/JustAnotherArchivist/snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d5edc-3cf2-4ffa-9322-8158e35c58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "query = \"(from:elonmusk) until:2023-02-17 since:2017-01-01\"\n",
    "# query = \"Bitcoins since:2018-01-01\" \n",
    "\n",
    "#On lance le chrono !\n",
    "start = time.time()\n",
    "\n",
    "tweets = []\n",
    "# Attention mettez le nombre max de Tweets !\n",
    "limit = 7000\n",
    "\n",
    "#utiliser snsscrape pour scraper les tweets et les mettre dans une liste\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([tweet.date, tweet.id,tweet.username, tweet.content, tweet.lang, \n",
    "                       tweet.hashtags, tweet.replyCount, tweet.retweetCount, tweet.likeCount,\n",
    "                      tweet.quoteCount, tweet.media, tweet.sourceLabel, tweet.coordinates,tweet.place])\n",
    "\n",
    "# créer un datframe qui contient tous les résultats\n",
    "df0 = pd.DataFrame(tweets, columns=['Date', 'TweetID', 'User', 'Tweet', 'Langue', 'Hashtags','ReplyCount','RetweetCount',\n",
    "                                  'LikeCount','Quotecount','Media', 'Source','coordinates','place'])\n",
    "\n",
    "#On vérifie le temps d'execution !\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f'Temps d\\'exécution : {elapsed:.2}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a29b2b-a26a-4682-9a55-dc86bf4737fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015311f-bb6b-434a-89ba-97ee690be193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2baa7e5-4f67-4668-b608-7b52b7d265f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f4b07-ae15-433a-be1f-1a3d3a2e6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['Source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfd160-b48e-4d8d-93be-1b2de7871342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['Langue'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6583c-9490-4085-afbe-7cff9f99c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a289099-8cc5-43fe-b441-09ef339f3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save to csv\n",
    "#df.to_csv('tweets_ElonMusk_Loubna_SERRAR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be965437-150a-4cb2-9838-10c3ca0216fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df0[['Date','User','Tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52a7f7-61be-4afc-9b93-f3e5c72890e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace659cc-5c78-4b3c-b486-2804b1e71d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Tweet'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4402d9-4b6b-47ba-bf67-3830433d4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737ff87-9f65-4ff7-8757-8cba189b2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "words = df['Tweet'].apply(lambda x:str(x).split())\n",
    "top=Counter([item for sublist in words for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0332f6-b462-47da-932f-f1285b41b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82902a2d-d5f0-434b-bbfa-7c5c6752a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token les plus fréquents\n",
    "top.most_common(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6409e44-60fc-427e-b9d1-7fb935fb85e5",
   "metadata": {},
   "source": [
    "# Pre-Processing Twitter Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7b685-f7ff-43f0-8241-9fe23749621a",
   "metadata": {},
   "source": [
    "## étape 0: Load necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5753a-4d55-4757-87d5-71936e8bbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt package\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dd8ad-2c45-4932-bf29-220001698271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for expanding contraction words e.g. isn't --> is not\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891fb89-b3b4-4b54-9541-79a8b97a3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b01ae6-c6a0-427c-82fb-daae49cb7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ac0a-b47d-4e75-b635-6d8647eabbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "stopwords.words('french')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893a438-279e-4e62-89e5-40d617345c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet lemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aab5b3-a4c7-4194-99cb-3342736b250b",
   "metadata": {},
   "source": [
    "## étape 1 : Basic Cleaning - à adapter à votre corpus/imagination !\n",
    "\n",
    "- Remove Unicode Strings and Noise\n",
    "- Remove/Replace URLs, User Mentions and Hashtags\n",
    "- Non-Letter characters: numbers, emojis, or hash marks.\n",
    "- Remove/Replace Slang and Abbreviations\n",
    "- Remove/Replace Contractions\n",
    "- Remove/Replace Numbers\n",
    "- Remove/Replace Repetitions of Punctuation\n",
    "- Remove Punctuation\n",
    "- Handling Capitalized Words / Lowercase\n",
    "- Replace Elongated Words (ex: hahahaaaa, ‘Duuuuude, that's awful,’”)\n",
    "\n",
    "https://pynative.com/python-regex-replace-re-sub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ff841-f605-4444-8ec7-db21dee29134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following pre-tokenization receives string as input parameter\n",
    "#and returns string as output\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet) # remove Twitter links\n",
    "    return tweet\n",
    "\n",
    "def remove_tags(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    tweet = re.sub('RT @[\\w_]+:','', tweet)  # remove retweet label\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "punctuation = '!”$%&\\’()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "def remove_nonText(tweet):\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub(r'\\n','', tweet)  # remove escape sequence\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('📝 …', '', tweet) # un exemple d'image que vous pouvez compléter !\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def remove_contraction(text):\n",
    "    return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "def pretokenization_cleaning(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "    tweet = remove_contraction(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf56d6-c3bb-44d4-9a34-7340a71fa009",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=df['Tweet'][3636]\n",
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253c35c-e3be-47fd-95ad-d1d5677cde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenization_cleaning(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb28a0-a75c-4443-b800-96e04c855126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe01a9-eb11-4223-ba0a-450f32ee73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling pretokenization_cleaning\n",
    "df['Clean']=[pretokenization_cleaning(sentence) for sentence in df['Tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be518af-a736-4f47-bb75-923068cbe2ee",
   "metadata": {},
   "source": [
    "## étape 2 : Normalising data  - à adapter à votre corpus \n",
    "- Spelling Correction\n",
    "- Replace Negations with Antonyms\n",
    "- Handling Capitalized Words\n",
    "- Lowercase\n",
    "- Tokenization\n",
    "- Remove Stopwords (ex: the, and….)\n",
    "- Stemming\n",
    "- Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07c7db-003c-4061-9b7b-3e3624fb279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    return tknzr.tokenize(text)\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([token for token in text if token.lower() not in stop_words])\n",
    "    #return [token for token in text if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd0300-416b-4ce8-94c1-09343cd2e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean and normalizing tweets, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "    tweet = remove_contraction(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = tokenize(tweet)  # apply tokenization\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504e2c2-4a26-4491-aba8-f0148e4b442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling pretokenization_cleaning\n",
    "df['Normalized']=[preprocess_tweet(sentence) for sentence in df['Tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cc9f0-4436-4fd6-99d6-3c5180bd175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all words\n",
    "all_words = ' '.join([word for word in df['Tweet']])\n",
    "all_Clean_words = ' '.join([word for word in df['Clean']])\n",
    "all_Normalized_words = ' '.join([word for word in df['Normalized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b59944-9619-4f52-8ce1-a203143f134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize all_words\n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
    "tokenized_Clean_words = nltk.tokenize.word_tokenize(all_Clean_words)\n",
    "tokenized_Normalized_words = nltk.tokenize.word_tokenize(all_Normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487d26f-6559-4ec7-a43c-28348f17db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "fdist_all = FreqDist(tokenized_words)\n",
    "fdist_clean = FreqDist(tokenized_Clean_words)\n",
    "fdist_normalized = FreqDist(tokenized_Normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ef4e4-2bd3-4b9b-a53a-a910d46641ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most frequent words in the corpus:\")\n",
    "print(fdist_all.most_common(20))\n",
    "print(\"The most frequent words in the corpus---- after cleaning the data:\")\n",
    "print(fdist_clean.most_common(20))\n",
    "print(\"The most frequent words in the corpus---- after normalizing the data!\")\n",
    "print(fdist_normalized.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe525c3-5e3f-4e68-b65c-4d74d213c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_normalized.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a502a9b-3aa2-4801-b4c2-dd2eff47044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78f6bd-9ca3-47ea-abe6-1662f2ddc47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eccc2e-737c-4940-a9d1-69b99e40e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating FreqDist for whole BoW, keeping the 20 most common tokens\n",
    "all_fdist = fdist_normalized.most_common(20)\n",
    "\n",
    "## Conversion to Pandas series via Python Dictionary for easier plotting\n",
    "all_fdist = pd.Series(dict(all_fdist))\n",
    "\n",
    "## Setting figure, ax into variables\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c0a39-b90d-4b57-9e8f-4fe4f8580786",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width= 1000, height = 600, max_words=200,\n",
    "                      random_state=1, background_color='White',\n",
    "                      collocations=False, stopwords = stop_words).generate(all_Normalized_words)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8ed80-fa75-4668-9ab8-123d5321c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d228b-833c-441e-b6bb-c8790d1f49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize(sentence):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    tokens=nltk.tokenize.word_tokenize(sentence)\n",
    "    return [WordNetLemmatizer().lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "#WordNetLemmatizer().lemmatize(token, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a0da2-e0b7-4829-8a39-c928eed075c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemmatized']=[lemmatize(sentence) for sentence in df['Normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017ac08-f67c-497a-b243-5ff7abfadf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cde91a-0153-4e9c-ba1f-d357920cbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for sentence in df['Lemmatized']:\n",
    "    df['Lemmatized_bis'][i]  = ' '.join(word for word in sentence)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b8654-279b-4183-80da-2e703e638cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a96d16-d3ea-4e83-8de8-3219d01f2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Lemmatized_words = ' '.join(words for words in df['Lemmatized_bis'])\n",
    "tokenized_Lemmatized_words = nltk.tokenize.word_tokenize(all_Lemmatized_words)\n",
    "fdist_Lemmatized = FreqDist(tokenized_Lemmatized_words)\n",
    "print(\"The most frequent lemma in the corpus---- after normalizing the data!\")\n",
    "print(fdist_Lemmatized.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664a280-3ac2-4e43-b1b8-379003d8c9a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Obtain top 10 words\n",
    "top_20 = fdist_Lemmatized.most_common(20)\n",
    "\n",
    "# Create pandas series to make plotting easier\n",
    "fdist = pd.Series(dict(top_20))\n",
    "\n",
    "sns.barplot(y=fdist.index, x=fdist.values, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a657e0d-e4b3-41bc-9f9c-c7dc6751e9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dbeb6d-9055-4c86-8105-4b645dd50799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
